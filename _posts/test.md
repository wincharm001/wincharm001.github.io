再梳理一遍：
基于 CLIP 的开放词汇分割模型的所有改进，都需要考虑的两个核心问题：精细化图像特征和 zero-shot 能力。现有不管是 training-based 还是 training-free 的研究（不完全调查）的大多数方法都较为复杂，且在最后精细化分割结果方面没有一个统一的范式。我们的研究就是从两个核心问题出发，研究一个简单、直接的 OVS 方法。
我们的方法的核心思路是：设计一个模块用于“修补”上采样得到的粗糙的相似度图。
具体来说，我们采用 CLIP 图像编码器中间层的特征 $F_{mid} \in \mathbb{R}^{N\times D}$，因为它们蕴含着丰富的边缘、轮廓等底层语义信息。通过简单的 MLP 结构进行学习映射，得到 K 个压缩后的向量 $b \in \mathbb{R}^{N \times K}$，将向量 $b$ 与相似度图 $s \in \mathbb{R}^{N \times C}$ 作矩阵乘法，得到一个权重矩阵 $b^\top \cdot s = W \in \mathbb{R}^{K \times C}$，这个矩阵保存着 K 个压缩向量到具体 C 个类别的权重信息。接下来分别调整形状并上采样 $b$（使用可学习的转置卷积上采样）和 $s$（使用线性插值上采样），记为 $B \in \mathbb{R}^{K\times H\times W}$ 和 $S\in \mathbb{R}^{C\times H\times W}$ ，$B$ 再经过简单的 MLP 映射调整输出，然后利用权重矩阵加权后，直接加到粗糙的相似度图 $S$ 上，得到最终的分割结果。
需要注意的是，我们如何在训练中防止我们的模块过拟合到 seen categories 上？我们需要选择合适的训练目标，来实现我们的方案。
1. 首先是传统的交叉熵损失用于保证最终的分割结果是正确、稳定的；
2. 其次就是为了保证不破坏 zero-shot 能力以及防止过拟合所专门设计的训练目标：
    - 这个目标的核心思路是：让得到的 $B$ 学习区分任意两个不同语义类别的区域。
    - 初步设计方案：对于任一训练样本图片，任意选取两个类别做区分，其余类别作背景，得到一个 3 类别的标签。同时为了统一输入与输出关系，我们需要对相似度图 $s$ 作同样的处理，只保留对应类别的区域激活。然后将标签和相似度图 one-hot 化并做差，得到偏差图标签 $B_{gt}$，使得 $B$ 可学习区分任意两个类别。在推理阶段，希望取得的结果是对于所有类别（包括训练过程中的 seen categories 和 unseen categories），我们的模块都能够很好地定位并精细化各类别的区域。
